name: Data Pipeline

on:
  workflow_dispatch:
    inputs:
      sources:
        description: "Comma-separated source IDs to run (empty = all)"
        required: false
        type: string
        default: ""
      limit:
        description: "Max records per source (0 = unlimited)"
        required: false
        type: string
        default: "0"
      force:
        description: "Force full re-sync and regeneration"
        required: false
        type: boolean
        default: false
      max_parallel:
        description: "Maximum number of concurrent partition jobs"
        required: false
        type: number
        default: 20
      skip_geocode:
        description: "Skip geocoding step in partition jobs"
        required: false
        type: string
        default: 'false'
      skip_ingest:
        description: "Skip ingest (sync/geocode/enrich) — only regenerate from existing R2 data"
        required: false
        type: boolean
        default: false
      merge_only:
        description: "Skip ingest + generation — pull existing partition outputs from R2 and just re-merge"
        required: false
        type: boolean
        default: false

concurrency:
  group: data-pipeline
  cancel-in-progress: false

env:
  RUST_LOG: crime_map=debug
  CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
  R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
  R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

jobs:
  # ── Step 0a: Discover sources and compute partition matrix ──────────
  setup:
    name: Compute Partitions
    runs-on: ubuntu-latest
    # Skip partition discovery entirely in merge_only mode
    if: ${{ !inputs.merge_only }}
    outputs:
      matrix: ${{ steps.partitions.outputs.matrix }}
      partition_count: ${{ steps.partitions.outputs.count }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Compute partition matrix
        id: partitions
        run: |
          # Discover all source TOML files and extract the `id` field from each.
          # Filenames often don't match the id (53 of 76 differ), so we must parse.
          SOURCE_DIR="packages/source/sources"
          ALL_SOURCES=""
          for toml in "$SOURCE_DIR"/*.toml; do
            sid=$(grep -m1 '^id\s*=' "$toml" | sed 's/^id\s*=\s*"\(.*\)"/\1/')
            if [ -n "$sid" ]; then
              ALL_SOURCES="$ALL_SOURCES $sid"
            else
              # Fallback to filename if no id field
              ALL_SOURCES="$ALL_SOURCES $(basename "$toml" .toml)"
            fi
          done
          ALL_SOURCES=$(echo "$ALL_SOURCES" | tr ' ' '\n' | grep -v '^$' | sort)

          # Load partition overrides if they exist
          OVERRIDES_FILE=".github/partition-overrides.json"
          GROUPED_SOURCES=""
          MATRIX_ENTRIES="[]"

          if [ -f "$OVERRIDES_FILE" ]; then
            echo "Loading partition overrides from $OVERRIDES_FILE"
            # Extract grouped sources and build matrix entries for groups
            GROUPED_SOURCES=$(jq -r '.groups | to_entries[] | .value[]' "$OVERRIDES_FILE" 2>/dev/null | sort)
            MATRIX_ENTRIES=$(jq -c '[.groups | to_entries[] | {name: .key, sources: (.value | join(","))}]' "$OVERRIDES_FILE" 2>/dev/null || echo "[]")
          fi

          # Add solo entries for sources not in any group
          SOLO_ENTRIES="[]"
          for source in $ALL_SOURCES; do
            if ! echo "$GROUPED_SOURCES" | grep -qx "$source"; then
              SOLO_ENTRIES=$(echo "$SOLO_ENTRIES" | jq -c ". + [{\"name\": \"$source\", \"sources\": \"$source\"}]")
            fi
          done

          # Merge grouped + solo entries
          FULL_MATRIX=$(echo "$MATRIX_ENTRIES" "$SOLO_ENTRIES" | jq -sc '.[0] + .[1]')

          # Apply source filter if specified (accepts both partition names and source IDs)
          FILTER="${{ inputs.sources }}"
          if [ -n "$FILTER" ]; then
            IFS=',' read -ra FILTER_NAMES <<< "$FILTER"
            FILTER_JSON=$(printf '%s\n' "${FILTER_NAMES[@]}" | jq -R . | jq -sc .)
            FULL_MATRIX=$(echo "$FULL_MATRIX" | jq -c --argjson filter "$FILTER_JSON" '[.[] | select((.name as $n | $filter | index($n)) or (.sources | split(",") | map(. as $s | $filter | index($s) // empty) | length > 0))]')
          fi

          COUNT=$(echo "$FULL_MATRIX" | jq 'length')
          echo "Computed $COUNT partition(s)"
          echo "$FULL_MATRIX" | jq .

          # Set outputs
          echo "matrix={\"include\":$(echo "$FULL_MATRIX" | jq -c .)}" >> "$GITHUB_OUTPUT"
          echo "count=$COUNT" >> "$GITHUB_OUTPUT"

  # ── Step 0b: Build Rust binaries once, share across all jobs ────────
  build:
    name: Build Binaries
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (with Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'true'
          rust-cache-key: pipeline-build

      - name: Build ingest binary
        uses: BSteffaniak/cache-artifact@master
        with:
          directory: ./packages
          command: |
            cargo build --release -p crime_map_ingest
            mkdir -p staging
            cp target/release/crime_map_ingest staging/crime_map_ingest
          output-path: staging/crime_map_ingest
          artifact-name: crime-map-ingest-binary
          cache-key-prefix: wt-crime-pipeline
          make-executable: true
          verify-command: --version

      - name: Build generate binary
        uses: BSteffaniak/cache-artifact@master
        with:
          directory: ./packages
          command: |
            cargo build --release -p crime_map_generate
            mkdir -p staging
            cp target/release/crime_map_generate staging/crime_map_generate
          output-path: staging/crime_map_generate
          artifact-name: crime-map-generate-binary
          cache-key-prefix: wt-crime-pipeline
          make-executable: true
          verify-command: --version

      - name: Upload pipeline binaries
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-binaries
          path: staging/
          retention-days: 7

  # ── Step 1a: Ingest census boundaries in parallel ───────────────────
  #
  # Each job writes to its own local boundaries.duckdb and pushes it to
  # R2 as boundaries-part/{type}.duckdb. No shared state between jobs.
  # Skipped in skip_ingest and merge_only modes.
  boundaries-ingest:
    name: "Boundaries (${{ matrix.type }})"
    runs-on: ubuntu-latest
    needs: build
    if: ${{ !inputs.skip_ingest && !inputs.merge_only }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - type: states
            command: crime_map_ingest states
          - type: counties
            command: crime_map_ingest counties
          # Tracts split into 5 chunks (~10 states each) for parallelism
          - type: tracts-1
            command: crime_map_ingest tracts --states=01,02,04,05,06,08,09,10,11,12
          - type: tracts-2
            command: crime_map_ingest tracts --states=13,15,16,17,18,19,20,21,22,23
          - type: tracts-3
            command: crime_map_ingest tracts --states=24,25,26,27,28,29,30,31,32,33
          - type: tracts-4
            command: crime_map_ingest tracts --states=34,35,36,37,38,39,40,41,42,44
          - type: tracts-5
            command: crime_map_ingest tracts --states=45,46,47,48,49,50,51,53,54,55,56
          # Places split into 5 chunks (~10 states each) for parallelism
          - type: places-1
            command: crime_map_ingest places --states=01,02,04,05,06,08,09,10,11,12
          - type: places-2
            command: crime_map_ingest places --states=13,15,16,17,18,19,20,21,22,23
          - type: places-3
            command: crime_map_ingest places --states=24,25,26,27,28,29,30,31,32,33
          - type: places-4
            command: crime_map_ingest places --states=34,35,36,37,38,39,40,41,42,44
          - type: places-5
            command: crime_map_ingest places --states=45,46,47,48,49,50,51,53,54,55,56
          - type: neighborhoods
            command: crime_map_ingest neighborhoods

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      - name: Pull cached boundary partition from R2
        run: crime_map_ingest pull-boundary-part --name ${{ matrix.type }}

      - name: "Ingest ${{ matrix.type }}"
        run: |
          CMD="${{ matrix.command }}"
          if [ "${{ inputs.force }}" = "true" ]; then
            CMD="$CMD --force"
          fi
          $CMD

      - name: Push boundary partition to R2
        run: crime_map_ingest push-boundary-part --name ${{ matrix.type }}

  # ── Step 1b: Merge boundary partitions from R2 ──────────────────────
  # Skipped in merge_only mode (boundaries already on R2).
  # Runs even when skip_ingest=true (boundaries-ingest was skipped) since
  # the merge reads from R2 partitions that were already cached.
  boundaries-merge:
    name: Merge Boundaries
    runs-on: ubuntu-latest
    needs: [build, boundaries-ingest]
    if: >-
      ${{
        !inputs.merge_only &&
        (needs.boundaries-ingest.result == 'success' ||
         needs.boundaries-ingest.result == 'skipped')
      }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      - name: Merge boundary partitions
        run: crime_map_ingest merge-boundaries

  # ── Step 1c: Generate boundary artifacts after merge ────────────────
  # Skipped in merge_only mode (boundary outputs already on R2).
  boundaries-generate:
    name: Generate Boundaries
    runs-on: ubuntu-latest
    needs: [build, boundaries-merge]
    if: >-
      ${{
        !inputs.merge_only &&
        needs.boundaries-merge.result == 'success'
      }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      - name: Pull shared databases from R2
        run: crime_map_ingest pull --shared-only

      - name: Generate boundary artifacts
        run: |
          mkdir -p data/boundaries
          crime_map_generate boundaries --output-dir data/boundaries --force

      - name: Push boundary outputs to R2
        run: crime_map_ingest push-generated-boundaries --dir data/boundaries

  # ── Step 2: Per-partition ingest + generate ─────────────────────────
  # In skip_ingest mode: pulls source DBs from R2, skips sync/geocode/enrich,
  # only regenerates outputs.
  # Skipped entirely in merge_only mode.
  partition:
    name: "${{ matrix.name }}"
    runs-on: ubuntu-latest
    needs: [setup, build, boundaries-merge]
    if: >-
      ${{
        !inputs.merge_only &&
        (needs.boundaries-merge.result == 'success' ||
         needs.boundaries-merge.result == 'skipped')
      }}
    strategy:
      fail-fast: false
      max-parallel: ${{ fromJSON(inputs.max_parallel || '20') }}
      matrix: ${{ fromJSON(needs.setup.outputs.matrix) }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      # Pull source DuckDBs (--sources-only to skip shared on first pull)
      - name: Pull source data from R2
        run: crime_map_ingest pull --sources ${{ matrix.sources }} --sources-only

      # Pull shared databases once (boundaries, geocode cache, geocoder index)
      - name: Pull shared databases from R2
        run: crime_map_ingest pull --shared-only

      - name: Unpack geocoder index (if available)
        if: ${{ !inputs.skip_ingest }}
        run: |
          if [ -f data/shared/geocoder_index.tar.zst ]; then
            crime_map_ingest geocoder-unpack
          else
            echo "No geocoder index archive found, skipping unpack"
          fi

      - name: Ingest sources
        if: ${{ !inputs.skip_ingest }}
        run: |
          INGEST_ARGS="sync-all --sources ${{ matrix.sources }}"
          if [ "${{ inputs.limit }}" != "0" ] && [ -n "${{ inputs.limit }}" ]; then
            INGEST_ARGS="$INGEST_ARGS --limit ${{ inputs.limit }}"
          fi
          if [ "${{ inputs.force }}" = "true" ]; then
            INGEST_ARGS="$INGEST_ARGS --force"
          fi
          crime_map_ingest $INGEST_ARGS

      # Push source DuckDB only (--sources-only avoids re-pushing shared DBs)
      - name: Push sync progress to R2
        if: ${{ !inputs.skip_ingest }}
        run: crime_map_ingest push --sources ${{ matrix.sources }} --sources-only

      - name: Geocode missing coordinates
        if: ${{ !inputs.skip_ingest && inputs.skip_geocode != 'true' }}
        run: crime_map_ingest geocode --sources ${{ matrix.sources }} --max-time 240
        env:
          PELIAS_URL: ${{ secrets.PELIAS_URL }}
          CF_ACCESS_CLIENT_ID: ${{ secrets.CF_ACCESS_CLIENT_ID }}
          CF_ACCESS_CLIENT_SECRET: ${{ secrets.CF_ACCESS_CLIENT_SECRET }}

      - name: Push geocode progress to R2
        if: ${{ !inputs.skip_ingest && inputs.skip_geocode != 'true' }}
        run: |
          crime_map_ingest push --sources ${{ matrix.sources }} --sources-only
          crime_map_ingest push --shared-only

      - name: Enrich spatial attribution
        if: ${{ !inputs.skip_ingest }}
        run: crime_map_ingest enrich --sources ${{ matrix.sources }}

      # Push source DuckDB after enrichment (--sources-only)
      - name: Push enriched data to R2
        if: ${{ !inputs.skip_ingest }}
        run: crime_map_ingest push --sources ${{ matrix.sources }} --sources-only

      - name: Generate artifacts
        run: |
          GENERATE_ARGS="all --skip-boundaries --sources ${{ matrix.sources }} --output-dir data/partitions/${{ matrix.name }}"
          if [ "${{ inputs.force }}" = "true" ]; then
            GENERATE_ARGS="$GENERATE_ARGS --force"
          fi
          crime_map_generate $GENERATE_ARGS

      # Final push of source DuckDB (--sources-only)
      - name: Push final state to R2
        if: ${{ !inputs.skip_ingest }}
        run: crime_map_ingest push --sources ${{ matrix.sources }} --sources-only

      - name: List generated files
        run: ls -lah data/partitions/${{ matrix.name }}/ 2>/dev/null || echo "No files generated"

      # Upload partition outputs to R2 (replaces GitHub Actions artifacts)
      - name: Push partition outputs to R2
        run: crime_map_ingest push-generated-partition --name ${{ matrix.name }} --dir data/partitions/${{ matrix.name }}

  # ── Step 3: Merge all partitions ────────────────────────────────────
  merge:
    name: Merge Artifacts
    runs-on: ubuntu-latest
    needs: [build, boundaries-generate, partition]
    if: >-
      ${{
        !inputs.merge_only &&
        needs.boundaries-generate.result == 'success' &&
        needs.partition.result == 'success'
      }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      # Pull boundary outputs from R2 (uploaded by boundaries-generate job)
      - name: Pull boundary outputs from R2
        run: |
          mkdir -p data/boundaries
          crime_map_ingest pull-generated-boundaries --dir data/boundaries

      # Pull all partition outputs from R2 (uploaded by partition jobs)
      - name: Pull partition outputs from R2
        run: |
          # List all partitions on R2 and pull each one
          PARTITIONS=$(crime_map_ingest list-generated-partitions 2>/dev/null | grep "^  " | tr -d ' ')
          for name in $PARTITIONS; do
            mkdir -p "data/partitions/$name"
            crime_map_ingest pull-generated-partition --name "$name" --dir "data/partitions/$name"
          done

      - name: List downloaded artifacts
        run: |
          echo "=== Boundaries ==="
          ls -lah data/boundaries/ 2>/dev/null || echo "none"
          echo "=== Partitions ==="
          find data/partitions -type f | head -100

      - name: Merge artifacts
        run: |
          # Build comma-separated list of partition dirs
          PARTITION_DIRS=$(find data/partitions -mindepth 1 -maxdepth 1 -type d | sort | tr '\n' ',' | sed 's/,$//')
          echo "Merging partition dirs: $PARTITION_DIRS"

          mkdir -p data/generated
          crime_map_generate merge \
            --partition-dirs "$PARTITION_DIRS" \
            --boundaries-dir data/boundaries \
            --output-dir data/generated

      - name: List merged files
        run: ls -lah data/generated/

      # Upload final merged outputs to R2
      - name: Push merged outputs to R2
        run: crime_map_ingest push-generated-merged --dir data/generated

  # ── Step 3 (merge_only): Pull existing outputs from R2 and re-merge ─
  merge-from-r2:
    name: Merge from R2
    runs-on: ubuntu-latest
    needs: [build]
    if: ${{ inputs.merge_only }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      # Pull boundary outputs from R2
      - name: Pull boundary outputs from R2
        run: |
          mkdir -p data/boundaries
          crime_map_ingest pull-generated-boundaries --dir data/boundaries

      # Pull partition outputs from R2 (optionally filtered by sources input)
      - name: Pull partition outputs from R2
        run: |
          FILTER="${{ inputs.sources }}"
          if [ -n "$FILTER" ]; then
            # Pull only the specified partitions
            IFS=',' read -ra NAMES <<< "$FILTER"
            for name in "${NAMES[@]}"; do
              name=$(echo "$name" | tr -d ' ')
              mkdir -p "data/partitions/$name"
              crime_map_ingest pull-generated-partition --name "$name" --dir "data/partitions/$name"
            done
          else
            # Pull all partitions from R2
            PARTITIONS=$(crime_map_ingest list-generated-partitions 2>/dev/null | grep "^  " | tr -d ' ')
            for name in $PARTITIONS; do
              mkdir -p "data/partitions/$name"
              crime_map_ingest pull-generated-partition --name "$name" --dir "data/partitions/$name"
            done
          fi

      - name: List downloaded artifacts
        run: |
          echo "=== Boundaries ==="
          ls -lah data/boundaries/ 2>/dev/null || echo "none"
          echo "=== Partitions ==="
          find data/partitions -type f | head -100

      - name: Merge artifacts
        run: |
          # Build comma-separated list of partition dirs
          PARTITION_DIRS=$(find data/partitions -mindepth 1 -maxdepth 1 -type d | sort | tr '\n' ',' | sed 's/,$//')
          echo "Merging partition dirs: $PARTITION_DIRS"

          mkdir -p data/generated
          crime_map_generate merge \
            --partition-dirs "$PARTITION_DIRS" \
            --boundaries-dir data/boundaries \
            --output-dir data/generated

      - name: List merged files
        run: ls -lah data/generated/

      # Upload final merged outputs to R2
      - name: Push merged outputs to R2
        run: crime_map_ingest push-generated-merged --dir data/generated
