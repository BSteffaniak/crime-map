name: Data Pipeline

on:
  workflow_dispatch:
    inputs:
      partitions:
        description: "Comma-separated partition names to run (empty = all)"
        required: false
        type: string
        default: ""
      limit:
        description: "Max records per source (0 = unlimited)"
        required: false
        type: string
        default: "0"
      force:
        description: "Force full re-sync and regeneration"
        required: false
        type: boolean
        default: false
      max_parallel:
        description: "Maximum number of concurrent partition jobs"
        required: false
        type: number
        default: 20

concurrency:
  group: data-pipeline
  cancel-in-progress: false

env:
  RUST_LOG: crime_map=debug
  CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
  R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
  R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

jobs:
  # ── Step 0a: Discover sources and compute partition matrix ──────────
  setup:
    name: Compute Partitions
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.partitions.outputs.matrix }}
      partition_count: ${{ steps.partitions.outputs.count }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Compute partition matrix
        id: partitions
        run: |
          # Discover all source TOML files and extract the `id` field from each.
          # Filenames often don't match the id (53 of 76 differ), so we must parse.
          SOURCE_DIR="packages/source/sources"
          ALL_SOURCES=""
          for toml in "$SOURCE_DIR"/*.toml; do
            sid=$(grep -m1 '^id\s*=' "$toml" | sed 's/^id\s*=\s*"\(.*\)"/\1/')
            if [ -n "$sid" ]; then
              ALL_SOURCES="$ALL_SOURCES $sid"
            else
              # Fallback to filename if no id field
              ALL_SOURCES="$ALL_SOURCES $(basename "$toml" .toml)"
            fi
          done
          ALL_SOURCES=$(echo "$ALL_SOURCES" | tr ' ' '\n' | grep -v '^$' | sort)

          # Load partition overrides if they exist
          OVERRIDES_FILE=".github/partition-overrides.json"
          GROUPED_SOURCES=""
          MATRIX_ENTRIES="[]"

          if [ -f "$OVERRIDES_FILE" ]; then
            echo "Loading partition overrides from $OVERRIDES_FILE"
            # Extract grouped sources and build matrix entries for groups
            GROUPED_SOURCES=$(jq -r '.groups | to_entries[] | .value[]' "$OVERRIDES_FILE" 2>/dev/null | sort)
            MATRIX_ENTRIES=$(jq -c '[.groups | to_entries[] | {name: .key, sources: (.value | join(","))}]' "$OVERRIDES_FILE" 2>/dev/null || echo "[]")
          fi

          # Add solo entries for sources not in any group
          SOLO_ENTRIES="[]"
          for source in $ALL_SOURCES; do
            if ! echo "$GROUPED_SOURCES" | grep -qx "$source"; then
              SOLO_ENTRIES=$(echo "$SOLO_ENTRIES" | jq -c ". + [{\"name\": \"$source\", \"sources\": \"$source\"}]")
            fi
          done

          # Merge grouped + solo entries
          FULL_MATRIX=$(echo "$MATRIX_ENTRIES" "$SOLO_ENTRIES" | jq -sc '.[0] + .[1]')

          # Apply partition filter if specified
          FILTER="${{ inputs.partitions }}"
          if [ -n "$FILTER" ]; then
            IFS=',' read -ra FILTER_NAMES <<< "$FILTER"
            FILTER_JSON=$(printf '%s\n' "${FILTER_NAMES[@]}" | jq -R . | jq -sc .)
            FULL_MATRIX=$(echo "$FULL_MATRIX" | jq -c --argjson filter "$FILTER_JSON" '[.[] | select(.name as $n | $filter | index($n))]')
          fi

          COUNT=$(echo "$FULL_MATRIX" | jq 'length')
          echo "Computed $COUNT partition(s)"
          echo "$FULL_MATRIX" | jq .

          # Set outputs
          echo "matrix={\"include\":$(echo "$FULL_MATRIX" | jq -c .)}" >> "$GITHUB_OUTPUT"
          echo "count=$COUNT" >> "$GITHUB_OUTPUT"

  # ── Step 0b: Build Rust binaries once, share across all jobs ────────
  build:
    name: Build Binaries
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (with Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'true'
          rust-cache-key: pipeline-build

      - name: Build release binaries
        run: cargo build --release -p crime_map_ingest -p crime_map_generate

      - name: Stage binaries for upload
        run: |
          mkdir -p staging
          cp target/release/crime_map_ingest staging/
          cp target/release/crime_map_generate staging/

      - name: Upload pipeline binaries
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-binaries
          path: staging/
          retention-days: 7

  # ── Step 1a: Ingest census boundaries in parallel ───────────────────
  #
  # Each job writes to its own local boundaries.duckdb and pushes it to
  # R2 as boundaries-part/{type}.duckdb. No shared state between jobs.
  boundaries-ingest:
    name: "Boundaries (${{ matrix.type }})"
    runs-on: ubuntu-latest
    needs: build
    strategy:
      fail-fast: false
      matrix:
        include:
          - type: states
            command: crime_map_ingest states
          - type: counties
            command: crime_map_ingest counties
          # Tracts split into 5 chunks (~10 states each) for parallelism
          - type: tracts-1
            command: crime_map_ingest tracts --states=01,02,04,05,06,08,09,10,11,12
          - type: tracts-2
            command: crime_map_ingest tracts --states=13,15,16,17,18,19,20,21,22,23
          - type: tracts-3
            command: crime_map_ingest tracts --states=24,25,26,27,28,29,30,31,32,33
          - type: tracts-4
            command: crime_map_ingest tracts --states=34,35,36,37,38,39,40,41,42,44
          - type: tracts-5
            command: crime_map_ingest tracts --states=45,46,47,48,49,50,51,53,54,55,56
          # Places split into 5 chunks (~10 states each) for parallelism
          - type: places-1
            command: crime_map_ingest places --states=01,02,04,05,06,08,09,10,11,12
          - type: places-2
            command: crime_map_ingest places --states=13,15,16,17,18,19,20,21,22,23
          - type: places-3
            command: crime_map_ingest places --states=24,25,26,27,28,29,30,31,32,33
          - type: places-4
            command: crime_map_ingest places --states=34,35,36,37,38,39,40,41,42,44
          - type: places-5
            command: crime_map_ingest places --states=45,46,47,48,49,50,51,53,54,55,56
          - type: neighborhoods
            command: crime_map_ingest neighborhoods

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      - name: "Ingest ${{ matrix.type }}"
        run: ${{ matrix.command }}

      - name: Push boundary partition to R2
        run: crime_map_ingest push-boundary-part --name ${{ matrix.type }}

  # ── Step 1b: Merge boundary partitions from R2 ──────────────────────
  boundaries-merge:
    name: Merge Boundaries
    runs-on: ubuntu-latest
    needs: [build, boundaries-ingest]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      - name: Merge boundary partitions
        run: crime_map_ingest merge-boundaries

  # ── Step 1c: Generate boundary artifacts after merge ────────────────
  boundaries-generate:
    name: Generate Boundaries
    runs-on: ubuntu-latest
    needs: [build, boundaries-merge]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      - name: Pull shared databases from R2
        run: crime_map_ingest pull --shared-only

      - name: Generate boundary artifacts
        run: |
          mkdir -p data/boundaries
          crime_map_generate boundaries --output-dir data/boundaries --force

      - name: Upload boundary artifacts
        uses: actions/upload-artifact@v4
        with:
          name: boundaries
          path: |
            data/boundaries/boundaries.pmtiles
            data/boundaries/boundaries.db
          retention-days: 7

  # ── Step 2: Per-partition ingest + generate ─────────────────────────
  partition:
    name: "${{ matrix.name }}"
    runs-on: ubuntu-latest
    needs: [setup, build, boundaries-merge]
    strategy:
      fail-fast: false
      max-parallel: ${{ fromJSON(inputs.max_parallel || '20') }}
      matrix: ${{ fromJSON(needs.setup.outputs.matrix) }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      - name: Pull data from R2
        run: |
          crime_map_ingest pull --sources ${{ matrix.sources }}
          crime_map_ingest pull --shared-only

      - name: Ingest sources
        run: |
          INGEST_ARGS="sync-all --sources ${{ matrix.sources }}"
          if [ "${{ inputs.limit }}" != "0" ] && [ -n "${{ inputs.limit }}" ]; then
            INGEST_ARGS="$INGEST_ARGS --limit ${{ inputs.limit }}"
          fi
          if [ "${{ inputs.force }}" = "true" ]; then
            INGEST_ARGS="$INGEST_ARGS --force"
          fi
          crime_map_ingest $INGEST_ARGS

      - name: Geocode missing coordinates
        run: crime_map_ingest geocode --sources ${{ matrix.sources }}
        env:
          PELIAS_URL: ${{ secrets.PELIAS_URL }}
          CF_ACCESS_CLIENT_ID: ${{ secrets.CF_ACCESS_CLIENT_ID }}
          CF_ACCESS_CLIENT_SECRET: ${{ secrets.CF_ACCESS_CLIENT_SECRET }}

      - name: Generate artifacts
        run: |
          GENERATE_ARGS="all --skip-boundaries --sources ${{ matrix.sources }} --output-dir data/partitions/${{ matrix.name }}"
          if [ "${{ inputs.force }}" = "true" ]; then
            GENERATE_ARGS="$GENERATE_ARGS --force"
          fi
          crime_map_generate $GENERATE_ARGS

      - name: Push data to R2
        run: crime_map_ingest push --sources ${{ matrix.sources }}

      - name: List generated files
        run: ls -lah data/partitions/${{ matrix.name }}/ 2>/dev/null || echo "No files generated"

      - name: Upload partition artifacts
        uses: actions/upload-artifact@v4
        with:
          name: partition-${{ matrix.name }}
          path: |
            data/partitions/${{ matrix.name }}/incidents.pmtiles
            data/partitions/${{ matrix.name }}/incidents.db
            data/partitions/${{ matrix.name }}/counts.duckdb
            data/partitions/${{ matrix.name }}/h3.duckdb
            data/partitions/${{ matrix.name }}/analytics.duckdb
            data/partitions/${{ matrix.name }}/metadata.json
          retention-days: 7

  # ── Step 3: Merge all partitions ────────────────────────────────────
  merge:
    name: Merge Artifacts
    runs-on: ubuntu-latest
    needs: [build, boundaries-generate, partition]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pipeline tools (no Rust)
        uses: ./.github/actions/setup-pipeline
        with:
          install-rust: 'false'

      - name: Download pipeline binaries
        uses: actions/download-artifact@v4
        with:
          name: pipeline-binaries
          path: /usr/local/bin

      - name: Make binaries executable
        run: chmod +x /usr/local/bin/crime_map_ingest /usr/local/bin/crime_map_generate

      - name: Download boundary artifacts
        uses: actions/download-artifact@v4
        with:
          name: boundaries
          path: data/boundaries

      - name: Download all partition artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: partition-*
          path: data/partitions

      - name: List downloaded artifacts
        run: |
          echo "=== Boundaries ==="
          ls -lah data/boundaries/ 2>/dev/null || echo "none"
          echo "=== Partitions ==="
          find data/partitions -type f | head -100

      - name: Merge artifacts
        run: |
          # Build comma-separated list of partition dirs
          PARTITION_DIRS=$(find data/partitions -mindepth 1 -maxdepth 1 -type d | sort | tr '\n' ',' | sed 's/,$//')
          echo "Merging partition dirs: $PARTITION_DIRS"

          mkdir -p data/generated
          crime_map_generate merge \
            --partition-dirs "$PARTITION_DIRS" \
            --boundaries-dir data/boundaries \
            --output-dir data/generated

      - name: List merged files
        run: ls -lah data/generated/

      - name: Upload merged artifacts
        uses: actions/upload-artifact@v4
        with:
          name: merged-data
          path: data/generated/
          retention-days: 14
